name: Docker Image CI

on:
  push:
    branches: [ master ]

jobs:

  push_to_registry:
    name: Push Docker image to Docker Hub
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        name: Check out code

      - uses: mr-smithers-excellent/docker-build-push@v5
        name: Build & push Spark master Docker image
        with:
          image:  ${{ secrets.DOCKER_HUB_USERNAME }}/${{ secrets.DOCKER_HUB_REPOSITORY }}
          tags: spark-master-3.1.1
          registry: docker.io
          dockerfile: ./master/Dockerfile
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      - uses: mr-smithers-excellent/docker-build-push@v5
        name: Build & push Spark worker Docker image
        with:
          image:  ${{ secrets.DOCKER_HUB_USERNAME }}/${{ secrets.DOCKER_HUB_REPOSITORY }}
          tags: spark-worker-3.1.1
          registry: docker.io
          dockerfile: ./worker/Dockerfile
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }} 


  test_sparke:
    needs: push_to_registry
    name: Test Spark and Hive metastore
    runs-on: ubuntu-latest
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: Create Docker network for docker-compose
        run: docker network create -d bridge spark

      - name: Start all containers via docker-compose
        run: docker-compose up -d
        
      - name: Wait for hive-server to be up 
        run: docker run willwill/wait-for-it localhost:10000 -- echo "Hive server is up"
        
      - name: Wait for Spark master to be up 
        run: docker run willwill/wait-for-it localhost:8080 -- echo "Spark master is up"        
        
      - name: Wait for Spark worker to be up 
        run: docker run willwill/wait-for-it localhost:8081 -- echo "Spark worker is up"    
        
      - name: Copy the test Python script into Spark master container
        run: docker cp test/test_spark.py spark-master:/tmp/test_spark.py
        
      - name: Spark submit the test script
        run: docker exec spark-master /spark/bin/spark-submit /tmp/test_spark.py
        
      - name: Check if db and table exists in Postgres
        run: docker exec hive-metastore-postgres psql -U hive -d metastore -c 'SELECT "TBL_NAME", "NAME" FROM "TBLS" JOIN "DBS" ON "TBLS"."DB_ID" = "DBS"."DB_ID";'        
        
        
